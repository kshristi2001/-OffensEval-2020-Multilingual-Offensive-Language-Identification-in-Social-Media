# -*- coding: utf-8 -*-
"""SubTaskB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFwlPToXPAfJl4pL4M7CVrC66DHPSVVt

# BERT MODEL FOR SUBTASK B
"""

# Importing all the necessary libraries
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#installing transformers
!pip install transformers

"""# LOADING THE OLID, TEST_LEVEL_B, LABELS-LEVELB DATA

"""

# Reading the labels data from a CSV file, using a tab separator and specifying column names.
labels_data = pd.read_csv("labels-levelb.csv", sep='\t', header=None, names=['combined'])

# Extracting 'id' and 'label' from the 'combined' column using string splitting.
labels_data['id'] = labels_data['combined'].str.split(',').str[0]
labels_data['label'] = labels_data['combined'].str.split(',').str[1]

# Dropping the 'combined' column as 'id' and 'label' have been extracted.
labels_data = labels_data.drop(columns=['combined'])

labels_data.head(10)

# Reading the test set data from a TSV file, using a tab separator and specifying column names.
testset_data = pd.read_csv("testset-levelb.tsv", sep='\t', header=0, names=['id', 'tweet'])
# Displaying the first 10 rows of the test set data.
testset_data.head(10)

# Reading the test set data from a TSV file, using a tab separator and specifying column names.
olid_data = pd.read_csv("olid-training-v1.0.tsv", sep='\t', header = 0,  names=['id', 'tweet', 'subtask_a', 'subtask_b','subtask_c'])
# Displaying the first 10 rows of the olid data set.
olid_data.head(5)

# Filling in empty or null values in the 'tweet' column of the olid_data DataFrame with an empty string.
olid_data['tweet'].fillna('', inplace=True)

# Filling in empty or null values in the 'tweet' column of the testset_data DataFrame with an empty string.
testset_data['tweet'].fillna('', inplace=True)

"""

```
# This is formatted as code
```

# PREPROCESSING USING NLTK LIBRARIES"""

# Importing necessary libraries from the Natural Language Toolkit (nltk).
import string
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Downloading required nltk resources.
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Defining a function for preprocessing tweets.
def preprocess_tweet(tweet):

    # Removing hashtags and @USER mentions from the tweet.
    tweet = re.sub(r'#\S+', '', tweet)
    tweet = re.sub(r'@USER', '', tweet)


    # Removing emojis using a regex pattern.
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F700-\U0001F77F"  # alchemical symbols
                               u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
                               u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
                               u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
                               u"\U0001FA00-\U0001FA6F"  # Chess Symbols
                               u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
                               u"\U00002702-\U000027B0"  # Dingbats
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    tweet = emoji_pattern.sub(r'', tweet)

    # Removing numbers from the tweet.
    tweet = re.sub(r'\d+', '', tweet)

    # Tokenizing the tweet and removing non-alphanumeric tokens.
    tokens = word_tokenize(tweet)
    tokens = [word for word in tokens if word.isalnum()]

    # Removing stop words and lemmatizing the tokens.
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.lower() not in stop_words]

    return ' '.join(tokens)

# Apply preprocessing to the tweet column in both olid_data and testset_data
olid_data['tweet'] = olid_data['tweet'].apply(preprocess_tweet)
testset_data['tweet'] = testset_data['tweet'].apply(preprocess_tweet)

#printing the datasets both the testset_data and olid_data
print("Testset Data:")
print(testset_data.head(5))
print("\nOLID Data:")
print(olid_data.head(5))

"""# DATA TOKENIZATION AND MODEL CONFIGURATION"""

# Initializing a BERT tokenizer from the 'bert-base-cased' pre-trained model.
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# Initializing a BERT model for sequence classification with 2 output labels i.e UNT and TIN
model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)

# Moving the BERT model to the specified device (GPU if available, otherwise CPU).
model = model.to(device)

print(model)

# Mapping label values to numeric form (0 for 'UNT' and 1 for 'TIN')
label_dict = {
    'UNT': 0,
    'TIN': 1
}

# Replacing label values in the 'subtask_b' column with their numeric counterparts
olid_data['subtask_b'] = olid_data['subtask_b'].replace(label_dict)

# Dropping rows with null values in the 'subtask_b' column
olid_data = olid_data.dropna(subset=['subtask_b'])

# Converting label values to a PyTorch tensor with integer data type
labels_train = torch.tensor(olid_data['subtask_b'].values.astype(int))

# Tokenizing and encoding the training data using the BERT tokenizer
encoded_data_train = tokenizer.batch_encode_plus(
    olid_data['tweet'].values,
    add_special_tokens=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=256,
    return_tensors='pt'
)

# Extracting input IDs and attention masks from the encoded training data
input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']

# Tokenizing and encoding the test set data using
#the BERT tokenizer
encoded_data_test = tokenizer.batch_encode_plus(
    testset_data['tweet'].values,
    add_special_tokens=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=256,
    return_tensors='pt'
)

# Extracting input IDs and attention masks from the encoded test set data
input_ids_test = encoded_data_test['input_ids']
attention_masks_test = encoded_data_test['attention_mask']

"""# DATA SET CREATION AND TRAINING THE MODEL"""

# Creating PyTorch datasets for training and testing
dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)

# For the test dataset, we only have input features (no labels)
dataset_test = TensorDataset(input_ids_test, attention_masks_test)

# Defining batch size
#for training and testing
batch_size = 32

# Creating PyTorch dataloaders for training and testing
dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)
dataloader_test = DataLoader(dataset_test, sampler=SequentialSampler(dataset_test), batch_size=batch_size)

"""# MODEL EVALUATION AND PREDICTION"""

model.eval()
predictions = []

# Use torch.no_grad() outside the loop for optimized memory usage
with torch.no_grad():
    # Process all batches at once with vectorization
    for batch in dataloader_test:
        # Move the entire batch to the device at once for better efficiency
        batch = tuple(b.to(device) for b in batch)

        inputs = {
            'input_ids': batch[0],
            'attention_mask': batch[1]
        }

        # Perform inference for the entire batch at once
        outputs = model(**inputs)

        logits = outputs[0]
        predictions.append(logits)

# Concatenate all predictions
predictions = torch.cat(predictions, dim=0).detach().cpu().numpy()

from sklearn.metrics import accuracy_score, classification_report

# Mapping labels to numeric values
label_dict = {
    'UNT': 0,
    'TIN': 1
}

labels_data_numeric = labels_data['label'].replace(label_dict)

# Converting labels to NumPy array
labels_data_numeric = labels_data_numeric.values.astype(int)

# Generating a classification report and calculating accuracy
report = classification_report(labels_data_numeric, np.argmax(predictions, axis=1), target_names=['UNT', 'TIN'], zero_division=1)
accuracy = accuracy_score(labels_data_numeric, np.argmax(predictions, axis=1))

# Printing accuracy and classification report
print("Accuracy on test data: {:.3f}%".format(accuracy * 100))
print("Classification Report:\n", report)

# Generating and visualizing the confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the confusion matrix
cm = confusion_matrix(labels_data_numeric, np.argmax(predictions, axis=1))


# Plotting the confusion matrix using seaborn and matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['UNT', 'TIN'], yticklabels=['UNT', 'TIN'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Importing necessary libraries for precision-recall curve and area under the curve (AUC) calculation
from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt

# Assuming y_true and y_scores are the true labels and predicted scores
precision, recall, _ = precision_recall_curve(labels_data_numeric, np.argmax(predictions, axis=1))
pr_auc = auc(recall, precision)

# Plotting the Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (area = {pr_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower right")
plt.show()

"""# MODEL EVALUATION AND PREDICATION USING SOLID 2020 DATASET"""

# Reading and processing labels of SOLID 2023 Dataset from a CSV file
lables_b_solid = pd.read_csv("test_b_labels_all.csv", sep='\t', header=None, names=['combined'])

# Extracting 'id' and 'label' from the combined column
lables_b_solid['id'] = lables_b_solid['combined'].str.split(',').str[0]
lables_b_solid['label'] = lables_b_solid['combined'].str.split(',').str[1]

# Dropping the 'combined' column
lables_b_solid = lables_b_solid.drop(columns=['combined'])

# Displaying the processed labels dataframe
lables_b_solid

# Reading the test set data of SOLID 2020 dataset from a TSV file, using a tab separator and specifying column names.
test_b_tweets_solid = pd.read_csv("test_b_tweets_all.tsv", sep='\t', header=0, names=['id', 'tweet'])
# Displaying the first 10 rows of the test set data.
test_b_tweets_solid.head(10)

# This will fill in the empty or null values in the data frames
test_b_tweets_solid['tweet'].fillna('', inplace=True)

# Apply preprocessing to the tweet column of solid dataset
test_b_tweets_solid['tweet'] = test_b_tweets_solid['tweet'].apply(preprocess_tweet)

#printing the preprocessed data
print(test_b_tweets_solid.head())

# Initializing a BERT tokenizer from the 'bert-base-cased' pre-trained model.
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# Initializing a BERT model for sequence classification with 2 output labels i.e UNT and TIN
model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)

# Moving the BERT model to the specified device (GPU if available, otherwise CPU).
model = model.to(device)

# Mapping label values to numeric form (0 for 'UNT' and 1 for 'TIN')
label_dict = {
    'UNT': 0,
    'TIN': 1
}

# Replacing label values in the 'subtask_b' column with their numeric counterparts
olid_data['subtask_b'] = olid_data['subtask_b'].replace(label_dict)

# Dropping rows with null values in the 'subtask_b' column
olid_data = olid_data.dropna(subset=['subtask_b'])

# Converting label values to a PyTorch tensor with integer data type
labels_train = torch.tensor(olid_data['subtask_b'].values.astype(int))

# Tokenizing and encoding the training data using the BERT tokenizer
encoded_data_train = tokenizer.batch_encode_plus(
    olid_data['tweet'].values,
    add_special_tokens=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=256,
    return_tensors='pt'
)

# Extracting input IDs and attention masks from the encoded training data
input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']

# Tokenizing and encoding the test set data using the BERT tokenizer
encoded_data_test = tokenizer.batch_encode_plus(
    test_b_tweets_solid['tweet'].values,
    add_special_tokens=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=256,
    return_tensors='pt'
)

# Extracting input IDs and attention masks from the encoded test set data
input_ids_test = encoded_data_test['input_ids']
attention_masks_test = encoded_data_test['attention_mask']

# Create dataset for training
dataset_train = TensorDataset(input_ids_train,attention_masks_train,labels_train)

# For test dataset, we only have features(no labels)
dataset_test = TensorDataset(input_ids_test,attention_masks_test)

# Defining the batch size for training and testing
batch_size = 32

# Creating PyTorch dataloaders for training and testing
dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)
dataloader_test = DataLoader(dataset_test, sampler=SequentialSampler(dataset_test), batch_size=batch_size)

# Set the model to evaluation mode
model.eval()

# Initialize an empty list to store predictions
predictions = []

# Iterate through batches in the test dataloader
for batch in dataloader_test:
    # Move the batch to the specified device (GPU or CPU)
    batch = tuple(b.to(device) for b in batch)

    # Prepare inputs for the model
    inputs = {
        'input_ids': batch[0],
        'attention_mask': batch[1]
    }

    # Perform forward pass without gradient computation
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract logits from the model outputs
    logits = outputs[0]

    # Append logits to the predictions list
    predictions.append(logits)

# Concatenate all predictions along the specified dimension
predictions = torch.cat(predictions, dim=0).detach().cpu().numpy()

from sklearn.metrics import accuracy_score, classification_report

# Define a dictionary to map label strings to numeric values
label_dict = {
    'UNT': 0,
    'TIN': 1
}

# Replace label strings with corresponding numeric values in the test data
labels_data_numeric = lables_b_solid['label'].replace(label_dict)

# Convert the labels to a NumPy array with integer data type
labels_data_numeric = labels_data_numeric.values.astype(int)

# Evaluate predictions using accuracy_score and classification_report
report = classification_report(labels_data_numeric, np.argmax(predictions, axis=1), target_names=['UNT', 'TIN'], zero_division=1)

# Calculate accuracy using accuracy_score
accuracy = accuracy_score(labels_data_numeric, np.argmax(predictions, axis=1))

# Print accuracy and classification report
print("Accuracy on test data: {:.3f}%".format(accuracy * 100))
print("Classification Report:\n", report)

# Generating and visualizing the confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the confusion matrix
cm = confusion_matrix(labels_data_numeric, np.argmax(predictions, axis=1))


# Plotting the confusion matrix using seaborn and matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['UNT', 'TIN'], yticklabels=['UNT', 'TIN'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Importing necessary libraries for precision-recall curve and area under the curve (AUC) calculation
from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt

# Assuming y_true and y_scores are the true labels and predicted scores
precision, recall, _ = precision_recall_curve(labels_data_numeric, np.argmax(predictions, axis=1))
pr_auc = auc(recall, precision)

# Plotting the Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (area = {pr_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower right")
plt.show()